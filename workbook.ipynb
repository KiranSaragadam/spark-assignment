{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3986d395-8d0e-4687-aee5-cb3bb882eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class start_process():\n",
    "    def __init__(self,file_name):\n",
    "        '''\n",
    "        initialte the spark session and load the file which is present in the directry\n",
    "        file_name: path to the csv file type string.\n",
    "        read the table with schema\n",
    "        schema\n",
    "        |-name -- string\n",
    "        |-sku -- string\n",
    "        |-description -- string\n",
    "        '''\n",
    "        import pyspark\n",
    "        self.spark = (pyspark.sql.SparkSession.builder\n",
    "                      .master(\"local\")\n",
    "                      .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")\n",
    "                      .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                      .config(\"spark.dynamicAllocation.enabled\",\"true\")\n",
    "                      .appName(\"Assignment\").getOrCreate())\n",
    "        self.schema = \"name string,sku string, description string\"\n",
    "        self.df0 = (self.spark.read\n",
    "                   .option(\"header\",True)\n",
    "                   .format(\"csv\")\n",
    "                   .option(\"multiline\",True)\n",
    "                   .option(\"delimiter\", \",\")\n",
    "                   .schema(self.schema)\n",
    "                   .load(file_name))\n",
    "        del(pyspark)\n",
    "        print('data read complete')\n",
    "    def update(self, name=None, sku=None, description=None, updatedata_path=None,insert=False):\n",
    "        '''\n",
    "        upsert the table either with the path of update dataframe or with the row values of name, sku, description.\n",
    "        default it will take path if not provided we need to give row values of name, sku, description.\n",
    "        need to pass the update table with schema\n",
    "        schema\n",
    "        |-name -- string\n",
    "        |-sku -- string\n",
    "        |-description -- string\n",
    "        want to insert the new rows if not there in dataframe pass insert = True.\n",
    "        '''\n",
    "        (self.df0.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(\"products\"))\n",
    "        print('data write complete')\n",
    "        from delta.tables import DeltaTable\n",
    "        df0 = DeltaTable.forPath(self.spark, \"products\")\n",
    "        if updatedata_path:\n",
    "            update = self.spark.read.format(\"delta\").load(updatedata_path)\n",
    "        elif (sku!=None and name!=None and description!=None):\n",
    "            update = (self.spark.sparkContext.parallelize([(name, sku, description)])\n",
    "                      .toDF(schema=\"name string,sku string, description string\"))\n",
    "        if insert:\n",
    "            (df0.alias(\"df0\")\n",
    "                .merge(update.alias(\"update\"), \"df0.sku = update.sku\")\n",
    "                .whenMatchedUpdate(set = \n",
    "                                   {\n",
    "                                       \"name\" : \"update.name\",\n",
    "                                       \"description\" : \"update.description\"\n",
    "                                   }\n",
    "                                  )\n",
    "                .whenNotMatchedInsert(values =\n",
    "                    {\n",
    "                      \"sku\": \"update.sku\",\n",
    "                      \"name\" : \"update.name\",\n",
    "                      \"description\":\"update.description\"\n",
    "                    }\n",
    "                                     )\n",
    "               ).execute()\n",
    "        else:\n",
    "            (df0.alias(\"df0\")\n",
    "                .merge(update.alias(\"update\"), \"df0.sku = update.sku\")\n",
    "                .whenMatchedUpdate(set = \n",
    "                                   {\n",
    "                                       \"name\" : \"update.name\",\n",
    "                                       \"description\" : \"update.description\"\n",
    "                                   }\n",
    "                                  )\n",
    "               )\n",
    "        del(update)\n",
    "        del(df0)\n",
    "        del(DeltaTable)\n",
    "        self.df0 = self.spark.read.format(\"delta\").load('products')\n",
    "        print(\"update complete\")\n",
    "    def save_table(self, db_name, table_name, db_location=None):\n",
    "        '''\n",
    "        save the table into the database provided with the location provided.\n",
    "        schema of output table\n",
    "        schema\n",
    "        |- name -- string\n",
    "        |- no_of_products -- bigint\n",
    "        '''\n",
    "        from pyspark.sql.functions import count\n",
    "        self.df1 = (self.df0\n",
    "                    .groupby(\"name\")\n",
    "                    .agg(count(\"*\").alias('no_of_products')))\n",
    "        self.table = table_name\n",
    "        if db_location:\n",
    "            self.db_location = db_location\n",
    "            self.spark.sql(\"CREATE DATABASE IF NOT EXISTS {} LOCATION '{}'\".format(db_name, db_location))\n",
    "            self.spark.sql(\"USE {}\".format(db_name))\n",
    "            self.df1.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            print(\"you table saved in\",db_name,\"database with table name\",table_name, \"in location\",db_location)\n",
    "        else:\n",
    "            self.db_location = None\n",
    "            self.db_name = db_name\n",
    "            self.spark.sql(\"CREATE DATABASE IF NOT EXISTS {} \".format(db_name))\n",
    "            self.spark.sql(\"USE {}\".format(db_name))\n",
    "            self.df1.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            print(\"you table saved in\",db_name,\"database with table name\",table_name)\n",
    "        del(count)\n",
    "        self.db = db_name\n",
    "    def clean(self,value=all):\n",
    "        '''\n",
    "        pass value='table' to delete table data.\n",
    "        pass value='db' to delete db data\n",
    "        pass value=all to remove all\n",
    "        default will be all\n",
    "        '''\n",
    "        import shutil\n",
    "        if value==all:\n",
    "            del(self.df0)\n",
    "            del(self.df1)\n",
    "            del(self.spark)\n",
    "            if self.db_location:\n",
    "                shutil.rmtree('spark-warehouse/{}'.format(self.db_location))\n",
    "            else:\n",
    "                shutil.rmtree('spark-warehouse/{}.db'.format(self.db_name))\n",
    "            self.spark.stop()\n",
    "        elif value=='db':\n",
    "            if self.db_location:\n",
    "                shutil.rmtree('spark-warehouse/{}'.format(self.db_location))\n",
    "            else:\n",
    "                shutil.rmtree('spark-warehouse/{}.db'.format(self.db_name))\n",
    "        elif value=='table':\n",
    "            if self.db_location:\n",
    "                shutil.rmtree('spark-warehouse/{}/{}'.format(self.db_location, self.table))\n",
    "            else:\n",
    "                shutil.rmtree('spark-warehouse/{}.db/{}'.format(self.db_name, self.table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c78870c-45c7-41ad-bed4-8852c1953660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data write complete\n",
      "update complete\n"
     ]
    }
   ],
   "source": [
    "table = start_process(\"products.csv\")\n",
    "table.update(name=\"hello\",sku='lay-raise-best-end',description='joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9df23743-4759-4861-af1f-ab2eb981aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you table saved in db_name database with table name table_name\n"
     ]
    }
   ],
   "source": [
    "table.save_table('db_name', 'table_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ccca0b-c10a-499d-8064-53ad846f3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.clean('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b39a30-a26a-4c35-a283-b1aaceda587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8ee683-2cc2-47ff-b053-f0a96f2e3fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read complete\n"
     ]
    }
   ],
   "source": [
    "table = spark_file.start_process(\"products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf5541-c26e-4717-bca1-471f0a3f01ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
